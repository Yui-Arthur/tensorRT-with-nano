{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "_a7RBVllTOCd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training yolov8 with dogs and cats dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/andrewmvd/dog-and-cat-detection"
      ],
      "metadata": {
        "id": "h6Ym_co7Wsl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### use kaggle api to download dataset\n",
        "\n",
        "#### upload kaggle api token to ``` ~/.kaggle/kaggle.json ```"
      ],
      "metadata": {
        "id": "Tt359aGMXSJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user kaggle"
      ],
      "metadata": {
        "id": "2DEcGanmXIFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d andrewmvd/dog-and-cat-detection\n",
        "!unzip ./dog-and-cat-detection.zip"
      ],
      "metadata": {
        "id": "b7vyugLmYjYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Pascal VOC XML to COCO JSON"
      ],
      "metadata": {
        "id": "QEuI2FqQmtnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def convert_label(path, lb_path):\n",
        "  def convert_box(size, box):\n",
        "    dw, dh = 1. / size[0], 1. / size[1]\n",
        "    x, y, w, h = (box[0] + box[1]) / 2.0 - 1, (box[2] + box[3]) / 2.0 - 1, box[1] - box[0], box[3] - box[2]\n",
        "    return x * dw, y * dh, w * dw, h * dh\n",
        "\n",
        "  in_file = open(path)\n",
        "  out_file = open(lb_path, 'w')\n",
        "  tree = ET.parse(in_file)\n",
        "  root = tree.getroot()\n",
        "  size = root.find('size')\n",
        "  w = int(size.find('width').text)\n",
        "  h = int(size.find('height').text)\n",
        "\n",
        "  names = [\"dog\" , \"cat\"] # names list\n",
        "  for obj in root.iter('object'):\n",
        "    cls = obj.find('name').text\n",
        "    if cls in names and int(obj.find('difficult').text) != 1:\n",
        "      xmlbox = obj.find('bndbox')\n",
        "      bb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\n",
        "      cls_id = names.index(cls)  # class id\n",
        "      out_file.write(\" \".join([str(a) for a in (cls_id, *bb)]) + '\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qqvdTa4wZDNN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path(\"./yolo\")\n",
        "\n",
        "for dir in (\"train\" , \"valid\"):\n",
        "  imgs_path = path / dir / 'images'\n",
        "  lbs_path = path / dir / 'labels'\n",
        "  imgs_path.mkdir(exist_ok=True, parents=True)\n",
        "  lbs_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "train_valid_ratio = 0.9\n",
        "annotations = list(Path(\"./annotations/\").glob(\"*.xml\"))\n",
        "for idx , an_path in enumerate(annotations):\n",
        "  target_lb_path = path / 'train' / 'labels' if idx < train_valid_ratio * len(annotations) else path / 'valid' / 'labels'\n",
        "  target_img_path = path / 'train' / 'images' if idx < train_valid_ratio * len(annotations) else path / 'valid' / 'images'\n",
        "  lb_path = (target_lb_path / an_path.name).with_suffix('.txt')  # new label path\n",
        "  shutil.copy( (Path(\"./images\") / an_path.name).with_suffix('.png') , target_img_path)\n",
        "  convert_label(an_path, lb_path)  # convert labels to YOLO format"
      ],
      "metadata": {
        "id": "ce3djSwbsxAB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_file = \"\"\"\n",
        "path: /content/yolo\n",
        "train: train/images\n",
        "val: valid/images\n",
        "\n",
        "names:\n",
        " 0: dog\n",
        " 1: cat\n",
        "\"\"\"\n",
        "with open(\"dog_cat.yaml\" , 'w') as f:\n",
        "  f.write(yaml_file)\n"
      ],
      "metadata": {
        "id": "GHhB7vGn8Ble"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cud1hVoJYMXQ"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train data=dog_cat.yaml model=yolov8n.pt epochs=10 imgsz=640"
      ],
      "metadata": {
        "id": "deXoZt6e-QE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### export yolov8 to onnx"
      ],
      "metadata": {
        "id": "orpE2xr2aRNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = Path(\"/content/runs/detect/train3/weights/\")\n",
        "pt_file = str(model_path / \"best.pt\")\n",
        "onnx_file = str(model_path / \"best.onnx\")\n",
        "trt_file = str(model_path / \"best.engine\")"
      ],
      "metadata": {
        "id": "myLH8_0ND6GR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model={pt_file} format=onnx"
      ],
      "metadata": {
        "id": "Jx8sSRFgYpCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### onnx to trt engine"
      ],
      "metadata": {
        "id": "zeyiJxKaWf2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade tensorrt"
      ],
      "metadata": {
        "id": "5r06djutgYtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
        "def build_engine(onnx_path, shape = [1,224,224,3]):\n",
        "\n",
        "   \"\"\"\n",
        "   This is the function to create the TensorRT engine\n",
        "   Args:\n",
        "      onnx_path : Path to onnx_file.\n",
        "      shape : Shape of the input of the ONNX file.\n",
        "  \"\"\"\n",
        "   with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, builder.create_builder_config() as config, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
        "       config.max_workspace_size = (256 << 20)\n",
        "       with open(onnx_path, 'rb') as model:\n",
        "           parser.parse(model.read())\n",
        "       network.get_input(0).shape = shape\n",
        "       engine = builder.build_engine(network, config)\n",
        "       return engine\n",
        "\n",
        "def save_engine(engine, file_name):\n",
        "   buf = engine.serialize()\n",
        "   with open(file_name, 'wb') as f:\n",
        "       f.write(buf)\n",
        "def load_engine(trt_runtime, plan_path):\n",
        "   with open(plan_path, 'rb') as f:\n",
        "       engine_data = f.read()\n",
        "   engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
        "   return engine"
      ],
      "metadata": {
        "id": "c6Y8UGeZiUAz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from onnx import ModelProto\n",
        "import tensorrt as trt\n",
        "\n",
        "engine_name = trt_file\n",
        "onnx_path = onnx_file\n",
        "batch_size = 1\n",
        "\n",
        "model = ModelProto()\n",
        "with open(onnx_path, \"rb\") as f:\n",
        "    model.ParseFromString(f.read())\n",
        "\n",
        "d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
        "d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
        "d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
        "shape = [batch_size , d0, d1 ,d2]\n",
        "engine = build_engine(onnx_path, shape= shape)\n",
        "save_engine(engine, engine_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quryc8b4ic0O",
        "outputId": "52757f6f-7fc3-404b-85be-0a41bdab2561"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d5c2a7eeabb5>:14: DeprecationWarning: Use set_memory_pool_limit instead.\n",
            "  config.max_workspace_size = (256 << 20)\n",
            "<ipython-input-15-d5c2a7eeabb5>:18: DeprecationWarning: Use build_serialized_network instead.\n",
            "  engine = builder.build_engine(network, config)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tensorRT inference yolov8 engine"
      ],
      "metadata": {
        "id": "cMYRsUH0aWXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda\n",
        "!wget https://www.akc.org/wp-content/uploads/2021/07/Cavalier-King-Charles-Spaniel-laying-down-indoors.jpeg -O ./dog.jpeg"
      ],
      "metadata": {
        "id": "_V08kVfTa4Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "\n",
        "def allocate_buffers(engine, batch_size, data_type):\n",
        "\n",
        "   \"\"\"\n",
        "   This is the function to allocate buffers for input and output in the device\n",
        "   Args:\n",
        "      engine : The path to the TensorRT engine.\n",
        "      batch_size : The batch size for execution time.\n",
        "      data_type: The type of the data for input and output, for example trt.float32.\n",
        "\n",
        "   Output:\n",
        "      h_input_1: Input in the host.\n",
        "      d_input_1: Input in the device.\n",
        "      h_output_1: Output in the host.\n",
        "      d_output_1: Output in the device.\n",
        "      stream: CUDA stream.\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "   # Determine dimensions and create page-locked memory buffers (which won't be swapped to disk) to hold host inputs/outputs.\n",
        "   h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))\n",
        "   h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))\n",
        "   # Allocate device memory for inputs and outputs.\n",
        "   d_input_1 = cuda.mem_alloc(h_input_1.nbytes)\n",
        "\n",
        "   d_output = cuda.mem_alloc(h_output.nbytes)\n",
        "   # Create a stream in which to copy inputs/outputs and run inference.\n",
        "   stream = cuda.Stream()\n",
        "   return h_input_1, d_input_1, h_output, d_output, stream\n",
        "\n",
        "def load_images_to_buffer(pics, pagelocked_buffer):\n",
        "   preprocessed = np.asarray(pics).ravel()\n",
        "   np.copyto(pagelocked_buffer, preprocessed)\n",
        "\n",
        "def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size, height, width):\n",
        "   \"\"\"\n",
        "   This is the function to run the inference\n",
        "   Args:\n",
        "      engine : Path to the TensorRT engine\n",
        "      pics_1 : Input images to the model.\n",
        "      h_input_1: Input in the host\n",
        "      d_input_1: Input in the device\n",
        "      h_output_1: Output in the host\n",
        "      d_output_1: Output in the device\n",
        "      stream: CUDA stream\n",
        "      batch_size : Batch size for execution time\n",
        "      height: Height of the output image\n",
        "      width: Width of the output image\n",
        "\n",
        "   Output:\n",
        "      The list of output images\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "   load_images_to_buffer(pics_1, h_input_1)\n",
        "\n",
        "   with engine.create_execution_context() as context:\n",
        "      # Transfer input data to the GPU.\n",
        "      cuda.memcpy_htod_async(d_input_1, h_input_1, stream)\n",
        "\n",
        "      # Run inference.\n",
        "\n",
        "      context.profiler = trt.Profiler()\n",
        "      context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])\n",
        "\n",
        "      # Transfer predictions back from the GPU.\n",
        "      cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
        "      # Synchronize the stream\n",
        "      stream.synchronize()\n",
        "      # Return the host output.\n",
        "      out = h_output.reshape((batch_size, 6, 8400))\n",
        "      # out = h_output\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "qcyVovAMac-V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_detect(img , x , y , width , height , conf , label):\n",
        "  # label = f'{CLASSES[class_id]} ({confidence:.2f})'\n",
        "  # color = colors[class_id]\n",
        "  print(x , y , width , height , conf , label)\n",
        "  cv2.rectangle(img, (x, y), (x + width, y + height), (0,0,255), 2)\n",
        "\n",
        "  cv2.putText(img, f\"{label} {conf}\", (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "def show_detect(img , preds , threshold = 0.5):\n",
        "  boxes = []\n",
        "  scores = []\n",
        "  class_ids = []\n",
        "\n",
        "  for pred_idx in range(preds.shape[2]):\n",
        "    pred = preds[0,:,pred_idx]\n",
        "    box = [pred[0] - 0.5*pred[2] , pred[1] - 0.5*pred[3] , pred[2] , pred[3]]\n",
        "    conf = pred[4:]\n",
        "    label = np.argmax(conf)\n",
        "    max_conf = np.max(conf)\n",
        "    # print(x , y , width , height , np.max(conf))\n",
        "    boxes.append(box)\n",
        "    scores.append(max_conf)\n",
        "    class_ids.append(label)\n",
        "\n",
        "  result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
        "\n",
        "  for i in range(len(result_boxes)):\n",
        "    index = result_boxes[i]\n",
        "    box = boxes[index]\n",
        "    detection = {\n",
        "            'class_id': class_ids[index],\n",
        "            # 'class_name': CLASSES[class_ids[index]],\n",
        "            'confidence': scores[index],\n",
        "            'box': box,\n",
        "            # 'scale': scale}\n",
        "    }\n",
        "    # detections.append(detection)\n",
        "    draw_detect(img, round(box[0]), round(box[1]),round(box[2]), round(box[3]),\n",
        "        scores[index] , class_ids[index])\n",
        "  cv2_imshow(img)\n",
        "    # print()\n",
        "    # break\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "E8HnEuTpRr9S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import tensorrt as trt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorrt as trt\n",
        "import torch.nn as nn\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
        "\n",
        "serialized_plan_fp32 = trt_file\n",
        "HEIGHT = 640\n",
        "WIDTH = 640\n",
        "\n",
        "img = cv2.imread(\"dog.jpeg\")\n",
        "img = cv2.resize(img , (WIDTH , HEIGHT))\n",
        "im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "im = np.array(im, dtype=np.float32, order='C')\n",
        "im = im.transpose((2, 0, 1))\n",
        "im = (2.0 / 255.0) * im - 1.0\n",
        "\n",
        "engine = load_engine(trt_runtime, serialized_plan_fp32)\n",
        "print(engine)\n",
        "h_input, d_input, h_output, d_output, stream = allocate_buffers(engine, 1, trt.float32)\n",
        "\n",
        "\n",
        "\n",
        "# cv2_imshow(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3durmcNST-ff",
        "outputId": "9d4422da-8b0f-4a7a-b6c5-21c03e66b614"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorrt_bindings.tensorrt.ICudaEngine object at 0x7ff146122fb0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-4b183685e721>:25: DeprecationWarning: Use get_tensor_shape instead.\n",
            "  h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))\n",
            "<ipython-input-24-4b183685e721>:26: DeprecationWarning: Use get_tensor_shape instead.\n",
            "  h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = do_inference(engine, im, h_input, d_input, h_output, d_output, stream, 1, HEIGHT, WIDTH)\n",
        "# cv2_imshow(img)\n",
        "show_detect(img , out)"
      ],
      "metadata": {
        "id": "Wssv8LaKVYF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}